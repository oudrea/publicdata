LOAD CSV WITH HEADERS FROM $publicURL + '/all_Bucket.csv' AS line MERGE (n:Bucket:Entity {guid: line.identifier, name: line.name, data_source: "KP", source_identifier: line.identifier, global_identifier: "KP__" + line.identifier});
LOAD CSV WITH HEADERS FROM $publicURL + '/all_Sector.csv' AS line MERGE (n:Sector:Entity {guid: line.identifier, name: line.name, data_source: "KP", source_identifier: line.identifier, global_identifier: "KP__" + line.identifier});
LOAD CSV WITH HEADERS FROM $publicURL + '/all_SuperSector.csv' AS line MERGE (n:SuperSector:Entity {guid: line.identifier, name: line.name, data_source: "KP", source_identifier: line.identifier, global_identifier: "KP__" + line.identifier});
LOAD CSV WITH HEADERS FROM $publicURL + '/all_Region.csv' AS line MERGE (n:Region:Entity {guid: line.identifier, name: line.name, data_source: "KP", source_identifier: line.identifier, global_identifier: "KP__" + line.identifier});
:auto LOAD CSV WITH HEADERS FROM $publicURL + '/all_TwitterHandle.csv' AS line CALL {WITH line MERGE (n:TwitterHandle:Entity {guid: line.identifier, handle:line.handle, data_source: "KP", source_identifier: line.identifier, global_identifier: "KP__" + line.identifier}) ON CREATE SET n.name = line.name ON MATCH SET n.name = line.name } IN TRANSACTIONS OF 1000 ROWS;
:auto LOAD CSV WITH HEADERS FROM $publicURL + '/all_Mention.csv' AS line CALL {WITH line MERGE (n:Mention:Entity {guid: line.identifier, name: line.name, popularity: line.popularity, wikipedia_url: "https://en.wikipedia.org/wiki/" + line.Wikipedia, data_source: "KP", source_identifier: line.identifier, global_identifier: "KP__" + line.identifier}) } IN TRANSACTIONS OF 1000 ROWS;
:auto LOAD CSV WITH HEADERS FROM $publicURL + '/all_Ticker.csv' AS line CALL {WITH line MERGE (n:Ticker:Entity {guid: line.identifier, name: line.name, countryCode: line.countryCode, type: line.type, capiq_id: line.CapitalIQ, data_source: "KP", source_identifier: line.identifier, global_identifier: "KP__" + line.identifier})} IN TRANSACTIONS OF 1000 ROWS;
// :auto LOAD CSV WITH HEADERS FROM $publicURL + '/all_Ticker.csv' AS line CALL {WITH line MERGE (n:Ticker:Entity {guid: line.identifier } ON CREATE n = {name: line.name, countryCode: line.countryCode, type: line.type, capiq_id: line.CapitalIQ, data_source: "KP", source_identifier: line.identifier, global_identifier: "KP__" + line.identifier} ON MATCH SET n += {name: line.name, countryCode: line.countryCode, type: line.type, capiq_id: line.CapitalIQ, data_source: "KP", source_identifier: line.identifier, global_identifier: "KP__" + line.identifier})} IN TRANSACTIONS OF 1000 ROWS;

:auto USING PERIODIC COMMIT 1000 LOAD CSV WITH HEADERS FROM $publicURL + '/all_Organization.csv' AS line WITH apoc.convert.fromJsonList(line.location) as locations, line WITH apoc.convert.fromJsonList(line.handles) as handles, locations, line CREATE (n:Organization:Entity {guid: line.identifier, name:line.name, handles: handles, description: line.description, webpage:line.webpage, status:line.status, type: line.type, capiq_id:line.CapitalIQ, data_source: "KP", source_identifier: line.identifier, global_identifier: "KP__" + line.identifier})-[:LOCATED_AT]->(m:Address:Entity {postalCode: locations[0].postalCode, country: locations[0].country, streetAddress: locations[0].streetAddress, state:locations[0].state, city: locations[0].city, data_source: "KP"});
MATCH (o:Organization) - [:LOCATED_AT] - (a:Address) SET a.guid = o.guid + '_ADDR_' + apoc.hashing.fingerprint(a) RETURN a;
MATCH (a:Address) SET a.source_identifier = a.guid, a.global_identifier = a.data_source + "__" + a.guid RETURN a;
:auto LOAD CSV WITH HEADERS FROM $publicURL + '/all_Person.csv' AS line CALL {WITH line MERGE (n:Person:Entity {guid: line.identifier, name: line.name, description: line.description, birthPlace: line.birthPlace, birthName: line.birthName, deathPlace: line.deathPlace, birthDate: line.birthDate, deathDate: line.deathDate, capiq_id: line.CapitalIQ, wikipedia_url: line.Wikipedia, data_source: "KP", source_identifier: line.identifier, global_identifier: "KP__" + line.identifier}) } IN TRANSACTIONS OF 1000 ROWS;
:auto USING PERIODIC COMMIT 1000 LOAD CSV WITH HEADERS FROM $publicURL + '/per_Person_relationship.csv' AS line WITH apoc.convert.fromJsonMap(line.relationship_self) as rd, line MATCH (n1 {guid: line.id}) WITH rd, line, n1 MATCH (n2 {guid: rd.id}) CALL apoc.merge.relationship(n1, toUpper(line.relationship_type), rd.meta, rd.meta, n2, {}) YIELD rel RETURN rel;
CREATE INDEX entity_guid IF NOT EXISTS FOR (n:Entity) ON (n.guid);
CREATE INDEX entity_sident IF NOT EXISTS FOR (n:Entity) ON (n.data_source, n.source_identifier);
CREATE INDEX entity_gident IF NOT EXISTS FOR (n:Entity) ON (n.global_identifier);
CREATE INDEX person_guid IF NOT EXISTS FOR (n:Person) ON (n.guid);
CREATE INDEX organization_guid IF NOT EXISTS FOR (n:Organization) ON (n.guid);
CREATE INDEX ticker_guid IF NOT EXISTS FOR (n:Ticker) ON (n.guid);
CREATE INDEX twitterhandle_guid IF NOT EXISTS FOR (n:TwitterHandle) ON (n.guid);
CREATE INDEX mention_guid IF NOT EXISTS FOR (n:Mention) ON (n.guid);
CREATE INDEX bucket_guid IF NOT EXISTS FOR (n:Bucket) ON (n.guid);
CREATE INDEX region_guid IF NOT EXISTS FOR (n:Region) ON (n.guid);
CREATE INDEX sector_guid IF NOT EXISTS FOR (n:Sector) ON (n.guid);
CREATE INDEX ssector_guid IF NOT EXISTS FOR (n:SuperSector) ON (n.guid);
:auto USING PERIODIC COMMIT 1000 LOAD CSV WITH HEADERS FROM $publicURL + '/per_Organization_relationship.csv' AS line WITH apoc.convert.fromJsonMap(line.relationship_self) as rd, line MATCH (n1:Entity {guid: line.id}) WITH rd, line, n1 MATCH (n2:Entity {guid: rd.id}) CALL apoc.merge.relationship(n1, toUpper(line.relationship_type), rd.meta, rd.meta, n2, {}) YIELD rel RETURN rel;
CREATE FULLTEXT INDEX nameAndDescription_fulltext FOR (n:Person|Organization|Region|Sector|SuperSector|Bucket|Ticker|TwitterHandle) ON EACH [n.name, n.description];
CREATE FULLTEXT INDEX name_fulltext FOR (n:Person|Organization|Region|Sector|SuperSector|Bucket|Mention|Ticker|TwitterHandle) ON EACH [n.name];